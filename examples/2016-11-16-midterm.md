### Exercise 1

1) Any subset of a spanning set is also spanning. T/F

  False. A spanning subset is such that any vector in V can be
  created out of a linear combination of vectors in that subset.

2) Any spanning set in a finite-dimensional vector space contains a basis. T/F

  True. A basis is said to span and be linearly independent. If the spanning
  set is linearly independent, it can be said to be a basis. If it is redundant,
  then it can be made such that it is both linearly independent and stay
  spanning. (Thereom 4.15)

3) In R^2, the subset {(1,1),(0,1),(1,0),(0,0)} is independent. T/F

  False. You could take (0,1)+(1,0) and make (1,1).

4) The product of two symmetric matrices (of same size) is also symmetric. T/F

  False. I initially had the assumption that a symmetric matrix had the same
  a_ii vectors through the diagonal line. However, that is untrue. Through a
  R^2 example of a symmetric matrix with distinct coordinates in the diagonal,
  you can see that the resultant is not symmetrical.

### Exercise 2

State and prove the Cayley-Hamilton thereom for 2x2 matrices.

\[ p(\lambda) = det(\lambdaI_n - A) \],

Such that if we replace \lambda by the matrix A, then we get a result of 0.

\[ p(A) = 0 \]

Proof:
\[
  p(\lambda) = det(\lambdaI_2 - A) \\
  = det([ \lambda && 0 \\ 0 && \lambda ] - [ a && b \\ c && d ]) \\
  = \lambda^2 - (a + d)\lamdda + ad - bc \\

  Substitute \lambda by A: \\

  = A^2 - tr(A)A + det(A) = 0 \\

  Expand and solve, you will get 0.
\]

### Exercise 3:

Thereom 1:

\[ dim(u + v) = dim(u) + dim(v) - dim(u \cup v) \]

Any finite-dimensional subspace u, together with another finite-dimensional
space v, will be the sum of the two dimensions minus the intersection of the
two subspaces (or any shared vectors).

Thereom 2:

Direct sum is defined by subspace:
  1) Subspace V = W + U
  2) And must not be redundant, or V \cup V is the zero vector.

Then we can see that the dimension of V is the the sum of the dimensions W and
U, given that their intersection is the zero vector, and has no dimension.

### Exercise 4:

part i.

We know that an invertible matrix has the property A^{-1}A = I. Also that a set
is said to be linearly independent if there are no redundant vectors in the
set, and no vector in the set can be created by any combination of any of the
other vectors in the set.

A linear combination of a linearly independent set will only equal 0, if all
scalars k_i are 0.

So we must prove that:
\[
  \sum\limits_{k=1}^n x_kAv_k = 0 \\
  A \sum\limits_{k=1}^n x_kv_k = 0 \\
\]

Since we know that $ \sum\limits_{k=1}^n x_kv_k = 0 $ for all $ x_k = 0$ , we
can see that A on the outside of the summation will also be zero for all
scalars $x_k = 0$.

part ii.

Again, a subset is said to span if any vector in the vector space can be
produced from a linear combination of the vectors in that set.

Let any $w \in V. And we know that $w = \sum\limits_{k=1}^n a_iv_i$.

Show that $w = \sum\limits_{k=1}^n a_iAv_i$.
\[
  w = \sum\limits_{k=1}^n a_iAv_i \\
  A^{-1}w = \sum\limits_{k=1}^n a_iv_i \\
\]
Which does not prove our thereom. Instead if we began our proof with
$w' = A^{-1}w$. We get:
\[
  w' = \sum\limits_{k=1}^n a_iAv_i \\
  w  = \sum\limits_{k=1}^n a_iv_i \\
\]

Thus this proves that $A_iv_i$ is also spanning.

### Exercise 5:

Given $\{ A \in M_n(R) : tr(A) = 0 \}$ prove that it is a subspace and with
justification, find its dimension.

We know that a subspace must fulfill 3 conditions:
  1) Must contain the zero vector.
  2) Must have closure under scalar multiplication. ( $ku \in W$ )
  3) Must have closure under addition. ( $u+v \in W$ )

Proof:

1) The zero vector is $ \matrixb{ 0 && ... \\ 0 && ... } $, and since our only
condition is that the $tr(A) = 0$, then it is clear that A contains the zero
vector.

2) Our trace condition can be written as $ \sum\limits_{i=1}^n a_{ii}$, so
$ka => k \sum\limits_{i=1}^n a_{ii} = k(0)$ so the condition is fulfilled.

3) $\sum\limits_{i=1}^n a_{ii} + \sum\limits_{i=1}^n b_{ii} = 0$ Thus, it is
closed under addition.

We let $a_{nn} = -1(a_{11} + ... + a_{n-1n-1}$. And then find the basis of A.

We show that for each scalar multiple $a_{ii}$ or $a_{jj}$ there exists a unique
matrix that forms a basis. Note that in the linear combinations with the
coefficient of coordinates $a_{ii}$ the $a_{nn} = 0$.

So we see that the basis for $A$ has $n^2 - 1$ vectors, and thus, has a
dimension of $n^2 - 1$

This can easily be solved through kernal recognition. But that will take some
time and nose to the grindstone work to complete.
